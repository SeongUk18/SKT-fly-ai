{"cells":[{"cell_type":"markdown","id":"ZfPRt6dvp-vq","metadata":{"id":"ZfPRt6dvp-vq"},"source":["# Required packages & your custum environment"]},{"cell_type":"code","execution_count":1,"id":"hB4l0jK3os_M","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9510,"status":"ok","timestamp":1721352184247,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"hB4l0jK3os_M","outputId":"9ba8a14b-0a5d-4cdb-b5ca-a814b74adde0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}],"source":["!pip install gymnasium"]},{"cell_type":"code","execution_count":2,"id":"t5RLz0TOoy95","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30113,"status":"ok","timestamp":1721352220035,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"t5RLz0TOoy95","outputId":"8236e3fb-572e-43a1-861f-7cdc14fef3ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"lEhYL1OKpch_","metadata":{"id":"lEhYL1OKpch_","executionInfo":{"status":"ok","timestamp":1721352235210,"user_tz":-540,"elapsed":11173,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["!cp -r drive/MyDrive/Test/SKT/gym_examples /content"]},{"cell_type":"markdown","id":"-uu9osLeqjSJ","metadata":{"id":"-uu9osLeqjSJ"},"source":["# Importations & constants"]},{"cell_type":"code","execution_count":4,"id":"be000272-b287-41c7-b5ed-1bd352af1a71","metadata":{"id":"be000272-b287-41c7-b5ed-1bd352af1a71","tags":[],"executionInfo":{"status":"ok","timestamp":1721352244270,"user_tz":-540,"elapsed":6077,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","\n","# Configuration paramaters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","epsilon = 1.0  # Epsilon greedy parameter\n","epsilon_min = 0.1  # Minimum epsilon greedy parameter\n","epsilon_max = 1.0  # Maximum epsilon greedy parameter\n","epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance\n","                                             # of random action being taken\n","batch_size = 16  # Size of batch taken from replay buffer\n","max_steps_per_episode = 60\n","max_episodes = 5000\n","\n","num_actions = 64"]},{"cell_type":"code","execution_count":5,"id":"FFVVSMNXp8W9","metadata":{"id":"FFVVSMNXp8W9","executionInfo":{"status":"ok","timestamp":1721352247272,"user_tz":-540,"elapsed":530,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["# Experience replay buffers\n","action_history = []\n","action_mask_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","\n","# Variables for counting over episodes\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 1000\n","# Number of frames for exploration\n","epsilon_greedy_frames = 10000.0\n","# Maximum replay length\n","max_memory_length = 500000\n","# Train the model after 4 actions\n","update_after_actions = 4\n","# How often to update the target network\n","update_target_network = 10000"]},{"cell_type":"markdown","id":"yW65jkIvqnKe","metadata":{"id":"yW65jkIvqnKe"},"source":["# Loading gym environment"]},{"cell_type":"code","execution_count":6,"id":"5bdb4550-1558-4ba7-a910-9faa34652511","metadata":{"id":"5bdb4550-1558-4ba7-a910-9faa34652511","tags":[],"executionInfo":{"status":"ok","timestamp":1721352253005,"user_tz":-540,"elapsed":505,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["env = gym.make('gym_examples:gym_examples/Reversi-v0', render_mode=\"text\")"]},{"cell_type":"markdown","id":"68262c3f","metadata":{"id":"68262c3f"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":7,"id":"224573a5","metadata":{"id":"224573a5","executionInfo":{"status":"ok","timestamp":1721352256962,"user_tz":-540,"elapsed":1085,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["# Function to preprocess the state\n","def preprocess_state(env_observ):\n","    st = torch.from_numpy(env_observ).squeeze()\n","    st = st.to(torch.int64)\n","    st = torch.nn.functional.one_hot(st,num_classes=3)\n","    st = st.permute(2, 0, 1)\n","    return st.to(torch.float32)"]},{"cell_type":"markdown","id":"plBCTNfiqt3n","metadata":{"id":"plBCTNfiqt3n"},"source":["# Model definition"]},{"cell_type":"code","execution_count":8,"id":"f85ef96c-cc48-426a-b2f3-12e002502bc9","metadata":{"id":"f85ef96c-cc48-426a-b2f3-12e002502bc9","tags":[],"executionInfo":{"status":"ok","timestamp":1721352260077,"user_tz":-540,"elapsed":661,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["class QModel(nn.Module):\n","    def __init__(self, num_actions):\n","        super(QModel, self).__init__()\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding='same')\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding='same')\n","        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(1152, 512)\n","        self.fc2 = nn.Linear(512, num_actions)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.conv1(x))\n","        x = nn.functional.relu(self.conv2(x))\n","        x = self.dropout(x)\n","        x = nn.functional.relu(self.conv3(x))\n","        x = self.flatten(x)\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        action = self.fc2(x)\n","        return action"]},{"cell_type":"code","execution_count":9,"id":"8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f","metadata":{"id":"8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f","executionInfo":{"status":"ok","timestamp":1721352262668,"user_tz":-540,"elapsed":3,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# The first model makes the predictions for Q-values which are used to\n","# make a action.\n","model = QModel(num_actions)\n","model.to(device)\n","\n","# Build a target model for the prediction of future rewards.\n","# The weights of a target model get updated every 10000 steps thus when the\n","# loss between the Q-values is calculated the target Q-value is stable.\n","model_target = QModel(num_actions)\n","model_target.to(device)\n","\n","loss_function = nn.SmoothL1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ip74_Tt4VW-O","executionInfo":{"status":"ok","timestamp":1721352269553,"user_tz":-540,"elapsed":1113,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}},"outputId":"b6f6464b-eb62-4144-89e7-49065895d6d2"},"id":"Ip74_Tt4VW-O","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","id":"MX7sCGp-sRVa","metadata":{"id":"MX7sCGp-sRVa"},"source":["# Policies"]},{"cell_type":"code","execution_count":11,"id":"1866419c-5bac-484e-b527-94b07c0087e4","metadata":{"id":"1866419c-5bac-484e-b527-94b07c0087e4","executionInfo":{"status":"ok","timestamp":1721352274175,"user_tz":-540,"elapsed":516,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["# Function to select an action\n","# model: the torch model to compuate action-state value (i.e., q-value)\n","# state: a torch tensor (3 x 8 x 8) of float32, which is output by preprocess_state\n","# mask: a 64-size array (np.array)\n","def get_greedy_epsilon(model, state, mask):\n","    global epsilon\n","\n","    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n","    if np.random.rand(1)[0] < epsilon:\n","        action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n","    else:\n","        with torch.no_grad():\n","            # add a batch axis\n","            state_tensor = state.unsqueeze(0)\n","            # compute the q-values\n","            q_values = model(state_tensor)\n","            # select the q-values of valid actions\n","            action = torch.argmax(\n","                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","    # decay epsilon\n","    epsilon -= epsilon_interval / epsilon_greedy_frames\n","    epsilon = max(epsilon, epsilon_min)\n","\n","    return action"]},{"cell_type":"code","execution_count":12,"id":"2e67c561-ffd1-4e28-977e-c285430e4d9c","metadata":{"id":"2e67c561-ffd1-4e28-977e-c285430e4d9c","executionInfo":{"status":"ok","timestamp":1721352277852,"user_tz":-540,"elapsed":528,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["def get_greedy_action(model, state, mask):\n","    global epsilon\n","\n","    with torch.no_grad():\n","        state_tensor = state.unsqueeze(0) # batch dimension\n","        q_values = model(state_tensor)\n","\n","        action = torch.argmax(\n","                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","    return action"]},{"cell_type":"markdown","id":"df3cbad6","metadata":{"id":"df3cbad6"},"source":["# Replay Buffer Management"]},{"cell_type":"code","execution_count":13,"id":"94cece0b-2d15-4061-afc5-e94b8cabf7a0","metadata":{"id":"94cece0b-2d15-4061-afc5-e94b8cabf7a0","executionInfo":{"status":"ok","timestamp":1721352281485,"user_tz":-540,"elapsed":540,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["# sample a batch of _batch_size from replay buffers\n","# return numpy.ndarrays\n","def sample_batch(_batch_size):\n","    # Get indices of samples for replay buffers\n","    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n","\n","    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n","    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n","    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n","    action_sample = np.array([action_history[i] for i in indices])\n","\n","    # action mask is the mask for the valid actions at the '''next''' state\n","    action_mask_sample = np.array([action_mask_history[i] for i in indices])\n","    done_sample = np.array([float(done_history[i]) for i in indices])\n","\n","    return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"]},{"cell_type":"code","execution_count":14,"id":"zDoOM1rFx7if","metadata":{"id":"zDoOM1rFx7if","executionInfo":{"status":"ok","timestamp":1721352284545,"user_tz":-540,"elapsed":2,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["def append_history(state, state_next, reward, action, action_mask, done):\n","    # Save actions and states in replay buffer\n","    action_history.append(action)\n","    action_mask_history.append(action_mask)\n","    state_history.append(state)\n","    state_next_history.append(state_next)\n","    rewards_history.append(reward)\n","    done_history.append(done)"]},{"cell_type":"code","execution_count":15,"id":"3829c226","metadata":{"id":"3829c226","executionInfo":{"status":"ok","timestamp":1721352288829,"user_tz":-540,"elapsed":512,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["# Function to update the Q-network\n","def update_network():\n","    # sample a batch of ...\n","    state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n","        sample_batch(batch_size)\n","\n","    # Convert numpy arrays to PyTorch tensors\n","    state_sample = torch.tensor(state_sample, dtype=torch.float32).to(device)\n","    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32).to(device)\n","    action_sample = torch.tensor(action_sample, dtype=torch.int64).to(device)\n","    action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64).to(device)\n","    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32).to(device)\n","    done_sample = torch.tensor(done_sample, dtype=torch.float32).to(device)\n","\n","    # Compute the target Q-values for the states\n","    with torch.no_grad():\n","        future_rewards = model_target(state_next_sample)\n","        #future_rewards = future_rewards.cpu()\n","\n","        # compute the q-value for the next state and the action maximizing the q-value\n","        # note: the action should be valid (i.e., mask is set to 1)\n","        max_q_values = torch.max(\n","            future_rewards + action_mask_sample * 100., # trick to select a valid action\n","            dim=1).values.detach() - 100.\n","\n","        # compute the target q-value\n","        # if the step was final, max_q_values should not be added\n","        # we assume that the negative return of the opposite player is the return of next step\n","        # that is, G(t) = r(t+1) - g*r(t+2) + g^2*r(t+3) - g^3*r(t+4) + ...\n","        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n","\n","    # It's forward propagation! Compute the Q-values for the taken actions\n","    q_values = model(state_sample)\n","    #q_values = q_values.cpu()\n","    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n","\n","    # Compute the loss\n","    loss = loss_function(q_values_action, target_q_values)\n","\n","    # Perform the optimization step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","id":"90ed4dcc-f24e-4990-879a-c2f6af51a063","metadata":{"id":"90ed4dcc-f24e-4990-879a-c2f6af51a063"},"source":["# Run DQN Tranining"]},{"cell_type":"code","execution_count":16,"id":"Kae3oT2ozMzK","metadata":{"id":"Kae3oT2ozMzK","executionInfo":{"status":"ok","timestamp":1721352293179,"user_tz":-540,"elapsed":589,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["# Experience replay buffers\n","action_history = []\n","action_mask_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []"]},{"cell_type":"code","execution_count":32,"id":"343adc5c-37d7-4429-890e-b720a291548c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"elapsed":13439,"status":"error","timestamp":1721352940780,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"343adc5c-37d7-4429-890e-b720a291548c","outputId":"816585f0-916c-47da-a96a-380bc78cd17f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 1270, Frame count: 37601, Running reward: -20.75\n","Episode: 1280, Frame count: 37899, Running reward: -17.78\n","Episode: 1290, Frame count: 38176, Running reward: -18.89\n","Episode: 1300, Frame count: 38475, Running reward: -16.43\n","Episode: 1310, Frame count: 38771, Running reward: -15.57\n","Episode: 1320, Frame count: 39045, Running reward: -12.43\n","Episode: 1330, Frame count: 39340, Running reward: -12.61\n"]},{"output_type":"error","ename":"AssertionError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-9cf999bf4baa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Take the selected action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mstate_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstate_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0maction_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gym_examples/envs/reversi_random.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAGENT_PLAYER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# -------------- fill here --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["for _ in range(max_episodes):\n","    state, info = env.reset()\n","    state = preprocess_state(state)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        frame_count += 1\n","\n","        # Select an action\n","        #state_cuda = state.to(device)\n","        action = get_greedy_epsilon(model,\n","                      state.to(device),\n","                      action_mask)\n","        if action < 0:\n","            print(action_mask)\n","\n","        # Take the selected action\n","        state_next, reward, done, _, info = env.step((action // 8, action % 8))\n","        state_next = preprocess_state(state_next)\n","        action_mask = info['action_mask'].reshape((-1,))\n","\n","        episode_reward += reward\n","\n","        # Store the transition in the replay buffer\n","        append_history(state, state_next, reward, action, action_mask, done)\n","\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n","            update_network()\n","\n","        if frame_count % update_target_network == 0:\n","            model_target.load_state_dict(model.state_dict())\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del action_mask_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","\n","    episode_count += 1\n","    episode_reward_history.append(episode_reward)\n","\n","    # Update running reward to check condition for solving\n","    if len(episode_reward_history) > 100:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    if episode_count % 10 == 0:\n","        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n","\n","    if episode_count % 5000 == 0:\n","        torch.save(model, 'model.{}'.format(episode_count))\n","    #if running_reward > 20:\n","    #    print(f\"Solved at episode {episode_count}!\")\n","    #    break\n","\n","\n","torch.save(model, 'model.final')"]},{"cell_type":"code","source":["env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROYwJvbZamcK","executionInfo":{"status":"ok","timestamp":1721352904815,"user_tz":-540,"elapsed":613,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}},"outputId":"a23b0f93-8764-45c4-b603-87ced55dc7a2"},"id":"ROYwJvbZamcK","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Current state of the board:\n","    0 1 2 3 4 5 6 7\n","-------------------\n","0 | 1 2 2 2 2 2 2 2\n","1 | 2 1 2 2 2 2 2 .\n","2 | 2 2 1 2 1 1 2 1\n","3 | 2 2 2 1 2 . 1 1\n","4 | 2 1 1 2 2 1 1 1\n","5 | 2 1 1 2 2 2 2 2\n","6 | 2 2 2 1 1 1 2 2\n","7 | 2 2 2 2 2 2 2 2\n","<class 'str'>\n"]}]},{"cell_type":"code","execution_count":40,"id":"4aad31b0-c081-4092-9414-484c98b70370","metadata":{"id":"4aad31b0-c081-4092-9414-484c98b70370","executionInfo":{"status":"ok","timestamp":1721353469553,"user_tz":-540,"elapsed":1004,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"outputs":[],"source":["torch.save(model.cpu().state_dict(), 'model')"]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgYA2pHAoeI2","executionInfo":{"status":"ok","timestamp":1721353475261,"user_tz":-540,"elapsed":527,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}},"outputId":"bcdb509a-4c59-490e-8139-59fad15418e7"},"id":"JgYA2pHAoeI2","execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gym_examples  model  sample_data\n"]}]},{"cell_type":"code","source":["!cp model drive/MyDrive/Test/SKT"],"metadata":{"id":"k112vUJmoolQ","executionInfo":{"status":"ok","timestamp":1721353478965,"user_tz":-540,"elapsed":4,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"}}},"id":"k112vUJmoolQ","execution_count":42,"outputs":[]},{"cell_type":"markdown","id":"4984b880-e427-48cb-bf91-13a91d6529f5","metadata":{"id":"4984b880-e427-48cb-bf91-13a91d6529f5"},"source":["# Evaluation (Agent vs. Gym's random play)"]},{"cell_type":"code","execution_count":null,"id":"5c198b4e-b0e4-4fd4-821d-be602c2dbc5a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31293,"status":"ok","timestamp":1721232367932,"user":{"displayName":"Younghoon Kim","userId":"02502586489806404813"},"user_tz":-540},"id":"5c198b4e-b0e4-4fd4-821d-be602c2dbc5a","outputId":"be8bc477-5a7c-48d0-af46-e1264a330ab2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current state of the board:\n","    0 1 2 3 4 5 6 7\n","-------------------\n","0 | 2 2 2 2 2 2 2 1\n","1 | 1 2 1 2 1 2 2 1\n","2 | 1 1 2 1 2 1 2 1\n","3 | 1 1 1 2 1 1 2 1\n","4 | 1 1 1 1 2 2 2 1\n","5 | 1 1 1 1 2 2 2 1\n","6 | 1 1 1 1 1 1 2 1\n","7 | 1 1 1 1 1 1 1 1\n","Game over! - Winner 1\n","<class 'str'>\n"]}],"source":["import time, sys\n","from IPython.display import clear_output\n","\n","board, info = env.reset()\n","state = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","done = False\n","env.render()\n","\n","while not done:\n","    action = get_greedy_action(model, state.to(device), action_mask)\n","    print(\"action: ({}, {})\".format(action // 8, action % 8))\n","    sys.stdout.flush()\n","\n","    time.sleep(1.0)\n","    clear_output(wait=False)\n","    board, reward, done, _, info = env.step((action // 8, action % 8))\n","    state = preprocess_state(board)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    env.render()"]},{"cell_type":"markdown","source":["# Evaluation (Agent vs. Human)"],"metadata":{"id":"cgKRLo18cnhe"},"id":"cgKRLo18cnhe"},{"cell_type":"code","execution_count":null,"id":"06aaaf66-05b5-4f6a-a004-5016908f8df4","metadata":{"id":"06aaaf66-05b5-4f6a-a004-5016908f8df4"},"outputs":[],"source":["board, info = env.reset()\n","state = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","done = False\n","env.render()\n","\n","while not done:\n","    action = get_greedy_action(model, state.to(device), action_mask)\n","    print(\"action: ({}, {})\".format(action // 8, action % 8))\n","    sys.stdout.flush()\n","\n","    time.sleep(1.0)\n","    clear_output(wait=False)\n","    board, reward, done, _, info = env.step((action // 8, action % 8))\n","    state = preprocess_state(board)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    env.render()"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}